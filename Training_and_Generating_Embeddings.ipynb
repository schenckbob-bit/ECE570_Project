{"cells":[{"cell_type":"code","source":["# Clone the diffusion model into our colab environment\n","!git clone https://github.com/lucidrains/denoising-diffusion-pytorch.git\n","%cd denoising-diffusion-pytorch/\n","!pip install -e .\n","!pip install pypng\n","\n","print(f\"Git Clone finished at {datetime.now()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D7EEUQHcdE-t","executionInfo":{"status":"ok","timestamp":1744470230909,"user_tz":240,"elapsed":19011,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"}},"outputId":"0abb9780-4c47-480b-c571-8d81097bcae3"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'denoising-diffusion-pytorch'...\n","remote: Enumerating objects: 1884, done.\u001b[K\n","remote: Total 1884 (delta 0), reused 0 (delta 0), pack-reused 1884 (from 1)\u001b[K\n","Receiving objects: 100% (1884/1884), 2.57 MiB | 30.93 MiB/s, done.\n","Resolving deltas: 100% (1336/1336), done.\n","/content/denoising-diffusion-pytorch/denoising-diffusion-pytorch\n","Obtaining file:///content/denoising-diffusion-pytorch/denoising-diffusion-pytorch\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (1.5.2)\n","Requirement already satisfied: einops in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (0.8.1)\n","Requirement already satisfied: ema-pytorch>=0.4.2 in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (0.7.7)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (2.0.2)\n","Requirement already satisfied: pillow in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (11.1.0)\n","Requirement already satisfied: pytorch-fid in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (0.3.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (1.14.1)\n","Requirement already satisfied: torch>=2.0 in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (2.6.0+cu124)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (0.21.0+cu124)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from denoising-diffusion-pytorch==2.1.1) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (3.18.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (4.13.1)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (2025.3.2)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2.0->denoising-diffusion-pytorch==2.1.1) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2.0->denoising-diffusion-pytorch==2.1.1) (1.3.0)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->denoising-diffusion-pytorch==2.1.1) (24.2)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate->denoising-diffusion-pytorch==2.1.1) (5.9.5)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.11/dist-packages (from accelerate->denoising-diffusion-pytorch==2.1.1) (6.0.2)\n","Requirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from accelerate->denoising-diffusion-pytorch==2.1.1) (0.30.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from accelerate->denoising-diffusion-pytorch==2.1.1) (0.5.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.21.0->accelerate->denoising-diffusion-pytorch==2.1.1) (2.32.3)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2.0->denoising-diffusion-pytorch==2.1.1) (3.0.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate->denoising-diffusion-pytorch==2.1.1) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate->denoising-diffusion-pytorch==2.1.1) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate->denoising-diffusion-pytorch==2.1.1) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.21.0->accelerate->denoising-diffusion-pytorch==2.1.1) (2025.1.31)\n","Installing collected packages: denoising-diffusion-pytorch\n","  Attempting uninstall: denoising-diffusion-pytorch\n","    Found existing installation: denoising-diffusion-pytorch 2.1.1\n","    Uninstalling denoising-diffusion-pytorch-2.1.1:\n","      Successfully uninstalled denoising-diffusion-pytorch-2.1.1\n","  Running setup.py develop for denoising-diffusion-pytorch\n","Successfully installed denoising-diffusion-pytorch-2.1.1\n","Collecting pypng\n","  Downloading pypng-0.20220715.0-py3-none-any.whl.metadata (13 kB)\n","Downloading pypng-0.20220715.0-py3-none-any.whl (58 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.1/58.1 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pypng\n","Successfully installed pypng-0.20220715.0\n","Git Clone finished at 2025-04-12 15:03:48.765306\n"]}]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1744470256571,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"},"user_tz":240},"id":"uLA6C287qxjj","outputId":"3103474d-1f3a-4efe-f7e1-10befde4b8e4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Can I can use GPU now? -- False\n","Imports finished at 2025-04-12 15:04:14.421964\n"]}],"source":["# Get the imports; check if the gpu is available\n","import time\n","from typing import List, Dict\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import torch.nn.functional as F\n","import torchvision\n","import torchvision.models as models\n","import torchvision.transforms as transforms\n","import os\n","import numpy as np\n","from PIL import Image\n","from datetime import datetime\n","import torch\n","from denoising_diffusion_pytorch import Unet, GaussianDiffusion, Trainer\n","import matplotlib.pyplot as plt\n","import pandas as pd\n","import io\n","import re\n","import struct\n","import png\n","from google.colab import files\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","print(f'Can I can use GPU now? -- {torch.cuda.is_available()}')\n","\n","print(f\"Imports finished at {datetime.now()}\")"]},{"cell_type":"code","source":["# NOTE: This model was trained using the ROCS dataset, to recreate the work, you will need to provide the same file I used\n","\n","print(f\"This is a manual interaction code section. Click the 'Choose Files' button, and please select the 'ROCS_winter2018.csv' file\")\n","uploaded = files.upload()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"TU_jzDBvdb4G","executionInfo":{"status":"ok","timestamp":1744470272317,"user_tz":240,"elapsed":8851,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"}},"outputId":"5e677794-3138-4662-d2db-4f71dead7eb6"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["This is a manual interaction code section. Click the 'Choose Files' button, and please select the 'ROCS_winter2018.csv' file\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["\n","     <input type=\"file\" id=\"files-5d3530e1-c8f0-46be-b029-f13efde90b5c\" name=\"files[]\" multiple disabled\n","        style=\"border:none\" />\n","     <output id=\"result-5d3530e1-c8f0-46be-b029-f13efde90b5c\">\n","      Upload widget is only available when the cell has been executed in the\n","      current browser session. Please rerun this cell to enable.\n","      </output>\n","      <script>// Copyright 2017 Google LLC\n","//\n","// Licensed under the Apache License, Version 2.0 (the \"License\");\n","// you may not use this file except in compliance with the License.\n","// You may obtain a copy of the License at\n","//\n","//      http://www.apache.org/licenses/LICENSE-2.0\n","//\n","// Unless required by applicable law or agreed to in writing, software\n","// distributed under the License is distributed on an \"AS IS\" BASIS,\n","// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n","// See the License for the specific language governing permissions and\n","// limitations under the License.\n","\n","/**\n"," * @fileoverview Helpers for google.colab Python module.\n"," */\n","(function(scope) {\n","function span(text, styleAttributes = {}) {\n","  const element = document.createElement('span');\n","  element.textContent = text;\n","  for (const key of Object.keys(styleAttributes)) {\n","    element.style[key] = styleAttributes[key];\n","  }\n","  return element;\n","}\n","\n","// Max number of bytes which will be uploaded at a time.\n","const MAX_PAYLOAD_SIZE = 100 * 1024;\n","\n","function _uploadFiles(inputId, outputId) {\n","  const steps = uploadFilesStep(inputId, outputId);\n","  const outputElement = document.getElementById(outputId);\n","  // Cache steps on the outputElement to make it available for the next call\n","  // to uploadFilesContinue from Python.\n","  outputElement.steps = steps;\n","\n","  return _uploadFilesContinue(outputId);\n","}\n","\n","// This is roughly an async generator (not supported in the browser yet),\n","// where there are multiple asynchronous steps and the Python side is going\n","// to poll for completion of each step.\n","// This uses a Promise to block the python side on completion of each step,\n","// then passes the result of the previous step as the input to the next step.\n","function _uploadFilesContinue(outputId) {\n","  const outputElement = document.getElementById(outputId);\n","  const steps = outputElement.steps;\n","\n","  const next = steps.next(outputElement.lastPromiseValue);\n","  return Promise.resolve(next.value.promise).then((value) => {\n","    // Cache the last promise value to make it available to the next\n","    // step of the generator.\n","    outputElement.lastPromiseValue = value;\n","    return next.value.response;\n","  });\n","}\n","\n","/**\n"," * Generator function which is called between each async step of the upload\n"," * process.\n"," * @param {string} inputId Element ID of the input file picker element.\n"," * @param {string} outputId Element ID of the output display.\n"," * @return {!Iterable<!Object>} Iterable of next steps.\n"," */\n","function* uploadFilesStep(inputId, outputId) {\n","  const inputElement = document.getElementById(inputId);\n","  inputElement.disabled = false;\n","\n","  const outputElement = document.getElementById(outputId);\n","  outputElement.innerHTML = '';\n","\n","  const pickedPromise = new Promise((resolve) => {\n","    inputElement.addEventListener('change', (e) => {\n","      resolve(e.target.files);\n","    });\n","  });\n","\n","  const cancel = document.createElement('button');\n","  inputElement.parentElement.appendChild(cancel);\n","  cancel.textContent = 'Cancel upload';\n","  const cancelPromise = new Promise((resolve) => {\n","    cancel.onclick = () => {\n","      resolve(null);\n","    };\n","  });\n","\n","  // Wait for the user to pick the files.\n","  const files = yield {\n","    promise: Promise.race([pickedPromise, cancelPromise]),\n","    response: {\n","      action: 'starting',\n","    }\n","  };\n","\n","  cancel.remove();\n","\n","  // Disable the input element since further picks are not allowed.\n","  inputElement.disabled = true;\n","\n","  if (!files) {\n","    return {\n","      response: {\n","        action: 'complete',\n","      }\n","    };\n","  }\n","\n","  for (const file of files) {\n","    const li = document.createElement('li');\n","    li.append(span(file.name, {fontWeight: 'bold'}));\n","    li.append(span(\n","        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n","        `last modified: ${\n","            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n","                                    'n/a'} - `));\n","    const percent = span('0% done');\n","    li.appendChild(percent);\n","\n","    outputElement.appendChild(li);\n","\n","    const fileDataPromise = new Promise((resolve) => {\n","      const reader = new FileReader();\n","      reader.onload = (e) => {\n","        resolve(e.target.result);\n","      };\n","      reader.readAsArrayBuffer(file);\n","    });\n","    // Wait for the data to be ready.\n","    let fileData = yield {\n","      promise: fileDataPromise,\n","      response: {\n","        action: 'continue',\n","      }\n","    };\n","\n","    // Use a chunked sending to avoid message size limits. See b/62115660.\n","    let position = 0;\n","    do {\n","      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n","      const chunk = new Uint8Array(fileData, position, length);\n","      position += length;\n","\n","      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n","      yield {\n","        response: {\n","          action: 'append',\n","          file: file.name,\n","          data: base64,\n","        },\n","      };\n","\n","      let percentDone = fileData.byteLength === 0 ?\n","          100 :\n","          Math.round((position / fileData.byteLength) * 100);\n","      percent.textContent = `${percentDone}% done`;\n","\n","    } while (position < fileData.byteLength);\n","  }\n","\n","  // All done.\n","  yield {\n","    response: {\n","      action: 'complete',\n","    }\n","  };\n","}\n","\n","scope.google = scope.google || {};\n","scope.google.colab = scope.google.colab || {};\n","scope.google.colab._files = {\n","  _uploadFiles,\n","  _uploadFilesContinue,\n","};\n","})(self);\n","</script> "]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saving ROCS_winter2018.csv to ROCS_winter2018.csv\n"]}]},{"cell_type":"code","source":["# This method is used to embed all of the example sentences into pngs as discussed in the paper\n","\n","df = pd.read_csv(io.BytesIO(uploaded['ROCS_winter2018.csv']))\n","\n","print(f\"Absobed input data. The shape of the data is {df.shape}\")\n","\n","sent1 = df['InputSentence1']\n","sent2 = df['InputSentence2']\n","sent3 = df['InputSentence3']\n","sent4 = df['InputSentence4']\n","\n","numSamples = len(sent1)\n","\n","# This will be explained in more detail later, but these are special words to identify the start and end of sentences\n","startWord = 'sss'\n","endWord = 'eee'\n","\n","# This method will be used to cut to the words of the sentence.\n","# Keeping apostrophes, but removing other punctuation\n","def tokenize(text: str) -> List[str]:\n","    word_re = r\"\\b[A-Za-z]+(?:'[A-Za-z]+)?\\b\"\n","    words = [w.lower() for w in re.findall(word_re, text)]\n","    return words\n","\n","# DEBUG ONLY\n","# example = \"He said, 'Isn't O'Brian the best?'\"\n","# print(example.split())\n","# print(tokenize(example))\n","\n","# This block for generating a word bank, there's only\n","# about 8500 words in the train data file that I've chosen\n","wordBank = {}\n","\n","# Word index is what we will use to look up words in the markov matrix\n","wordIndex = {}\n","reverseIndex = {}\n","indexIterator = 0\n","\n","# Read through all the sentences\n","for i in range(numSamples):\n","\n","  sent1Words = tokenize(sent1[i])\n","  sent2Words = tokenize(sent2[i])\n","  sent3Words = tokenize(sent3[i])\n","  sent4Words = tokenize(sent4[i])\n","\n","  allSentences = [sent1Words, sent2Words, sent3Words, sent4Words]\n","  for sentence in allSentences:\n","    for j in range(len(sentence)):\n","\n","      # Count every word, we're going to use the most frequent words\n","      if sentence[j] not in wordBank:\n","        wordBank[sentence[j]] = 1\n","        wordIndex[sentence[j]] = indexIterator\n","        reverseIndex[indexIterator] = sentence[j]\n","        indexIterator += 1\n","      else:\n","        wordBank[sentence[j]] += 1\n","\n","# Add the start word and the end word\n","wordBank[startWord] = 1\n","wordIndex[startWord] = indexIterator\n","indexIterator += 1\n","\n","wordBank[endWord] = 1\n","wordIndex[endWord] = indexIterator\n","indexIterator += 1\n","\n","# For testing\n","#print(wordBank)\n","#print(f\"The length of the wordbank is {len(wordBank)}\")\n","#print(f\"The length of the word index is {indexIterator}\")\n","\n","output_folder = './sentenceImages/'\n","if not os.path.exists(output_folder):\n","        os.makedirs(output_folder)\n","\n","\n","# When provided a list of tokenized words, this method will save them to a png file\n","def convertSentenceToBytes(tokenizedSentence: List[str], fileNumber) -> List[bytes]:\n","  # Every word in each sentence should be 2 bytes long. We're going to find the number in that each word translates to in the sentence, and break it into 2 bytes\n","  zeroString = '0000000000000000'\n","\n","  # The dimenstions of the image that we're going to make\n","  height = 12\n","  width = 12\n","  channels = 1\n","  byteArray = np.zeros(height * width*channels, dtype = np.uint8 )\n","  imageTensor = np.zeros((height, width, channels), dtype = np.uint8)\n","  index = 0\n","\n","  for word in tokenizedSentence:\n","    wordNumber = wordIndex[word]\n","    wordBytes = bin(wordNumber)[2:]\n","    wordBytes = zeroString[:(16-len(wordBytes))] + wordBytes\n","    firstByteString = wordBytes[:8]\n","    secondByteString = wordBytes[8:]\n","    #print(f\"Word: {word}; wordNumber: {wordNumber}; wordBytes: {wordBytes}; length: {len(wordBytes)}; firstByte: {firstByteString}; secondByte: {secondByteString}\")\n","\n","    firstByte = struct.pack('B',int(firstByteString, 2))\n","    secondByte = struct.pack('B', int(secondByteString, 2))\n","\n","    #print(f\"First Byte: {firstByte}; Second Byte: {secondByte}\")\n","\n","    byteArray[index] = int(firstByteString, 2)\n","    index += 1\n","    byteArray[index] = int(secondByteString, 2)\n","    index += 1\n","\n","  img = Image.fromarray(byteArray, mode='L')  # 'L' mode for grayscale images\n","\n","  postReadingBytes = io.BytesIO()\n","\n","  img.save(postReadingBytes,format='PNG' )\n","\n","  imgData = postReadingBytes.getvalue()\n","\n","  #print(byteArray)\n","  #print(imgData)\n","\n","  filePath = \"sentenceImages/sentenceGroup\" + str(fileNumber) + \".png\"\n","  img.save(filePath)\n","\n","\n","# Loop over all sentences and generate images\n","for i in range(numSamples):\n","\n","  sent1Words = tokenize(sent1[i])\n","  sent2Words = tokenize(sent2[i])\n","  sent3Words = tokenize(sent3[i])\n","  sent4Words = tokenize(sent4[i])\n","\n","  allSentences = []\n","  allSentences.extend(sent1Words)\n","  allSentences.extend(sent2Words)\n","  allSentences.extend(sent3Words)\n","  allSentences.extend(sent4Words)\n","  convertSentenceToBytes(allSentences, i)\n","\n","print(f\"Text to images completed at: {datetime.now()}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bKSh2W0zfdtC","executionInfo":{"status":"ok","timestamp":1744470830439,"user_tz":240,"elapsed":1799,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"}},"outputId":"85c0ab4c-51bb-4bfb-87ca-7bb5fb4a45ac"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Absobed input data. The shape of the data is (3142, 8)\n","Text to images completed at: 2025-04-12 15:13:48.260691\n"]}]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["ccd65b514ca04c3aa176bb51bd1be7b7","c737dc8bd3294d87a8ff5a4030ab91f4","e666312564394e11aab3b63fef319ddf","07737e04105b4636a3854763d9635569","0f558675fdeb4b8bb423728c76906b22","ca2735f1adee43cca5f492a4db4475ef","74e63f3a36bf481fa6b96e5fa0f75bb4","f8a1101aff1e49498157a799275aa6fc","70be187c50114b4284b3be858740bd38","3ac3d064483f49bfb291934fa0dacc04","3d65d56ae8f04d159a2bba6a20cd055b"]},"executionInfo":{"elapsed":298548,"status":"ok","timestamp":1744476903316,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"},"user_tz":240},"id":"jYKY-sGJdGdW","outputId":"97ad141e-fc47-44ab-d92a-f1ef49b693c4"},"outputs":[{"output_type":"display_data","data":{"text/plain":["  0%|          | 0/100 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ccd65b514ca04c3aa176bb51bd1be7b7"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["training complete\n","2025-04-12 16:55:00.741047\n"]}],"source":["# Use this cell to train\n","\n","# Specify the output folder\n","batch_size = 32\n","\n","model = Unet(\n","    dim = 64,\n","    dim_mults = (1, 2, 4),\n","    channels = 1\n",")\n","\n","diffusion = GaussianDiffusion(\n","    model,\n","    image_size = 12,\n","    timesteps = 1000    # number of steps; decrease this to speedup, quality will suffer\n",")\n","\n","# Define the trainer\n","\n","trainer = Trainer(\n","    diffusion,\n","    output_folder,\n","    train_batch_size = batch_size,\n","    train_lr = 2e-4,\n","    train_num_steps = 100000,  # Total training steps; decrease this to speedup, quality will suffer\n","    gradient_accumulate_every = 2,\n","    ema_decay = 0.995,\n","    amp = True,\n","    calculate_fid = False\n",")\n","\n","trainer.train()\n","\n","print(datetime.now())"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1744470818278,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"},"user_tz":240},"id":"y1rn4d_G8fAd","outputId":"e79e2ceb-928b-4f47-f178-181d92e37807"},"outputs":[{"output_type":"stream","name":"stdout","text":["The directory: ./sentenceImages/ does not exist\n"]}],"source":["import shutil\n","if os.path.exists(output_folder):\n","    shutil.rmtree(output_folder)\n","else:\n","    print(f\"The directory: {output_folder} does not exist\")"]},{"cell_type":"code","execution_count":24,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/","height":84,"referenced_widgets":["a3994852cc2544e9a3a1e11faa5d7c60","441f991917fb4a41a76351b596f75630","db8774b4b2744bd2ac76efba28ab4730","c809daf92bb14a198419e3aa277a17ef","f7f47958e3c74fb38bf239c55b28e97c","912e03ee280a462399c3b0f5be8a824c","5b35b84d81d840ffa8f5afb7be582767","050171d68a2c4b4387cfeaa704a4be76","a6ee20d6db844610b943b3e6c1216640","3a6b05aabdb64fcfaec520fa8c936332","61dc1a14d4a948d1acfa0c2b1f26747c"]},"id":"CBGfNNBidD1V","executionInfo":{"status":"ok","timestamp":1744477246095,"user_tz":240,"elapsed":255,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"}},"outputId":"183f1e5a-fba3-47b5-c7c7-ef6baf2f9875"},"outputs":[{"output_type":"display_data","data":{"text/plain":["sampling loop time step:   0%|          | 0/5 [00:00<?, ?it/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a3994852cc2544e9a3a1e11faa5d7c60"}},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Saved ./Generated/image_0.png\n","Image generated: 2025-04-12 17:00:43.496299\n"]}],"source":["# Use this cell to generate\n","\n","samples = diffusion.sample(batch_size = 1)\n","samples.shape # (1, 1, 12, 12)\n","\n","genFolder = \"./Generated/\"\n","\n","if not os.path.exists(genFolder):\n","        os.makedirs(genFolder)\n","\n","for i in range(samples.size(0)):\n","        # Get the image tensor and convert it to a numpy array\n","        img_tensor = samples[i]\n","        img_array = img_tensor.numpy()\n","\n","        img_array = (img_array * 255).astype(np.uint8)\n","        # print(img_array.shape)\n","        # print(img_array)\n","\n","        # Convert the numpy array to a PIL Image\n","        img = Image.fromarray(img_array[0], mode='L')  # 'L' mode for grayscale images\n","\n","            # Define the file path and save the image\n","        file_path = os.path.join(genFolder, f'image_0.png')\n","        img.save(file_path)\n","        print(f'Saved {file_path}')\n","\n","\n","print(f\"Image generated: {datetime.now()}\")\n",""]},{"cell_type":"code","execution_count":25,"metadata":{"scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"id":"TPtQU7PadD1V","executionInfo":{"status":"ok","timestamp":1744477248717,"user_tz":240,"elapsed":10,"user":{"displayName":"Bob Schenck","userId":"06456904815889268999"}},"outputId":"75855192-456a-4afe-b37d-8820ef01532e"},"outputs":[{"output_type":"stream","name":"stdout","text":[" JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT JUNK_WORD_OUT_OF_CONTEXT nation ruth gloves nacho wound fliers scaling frozen it'd improved town's scoured energetic strawberries cup troubled troubled counselors ignored cup crab a troubled received admire weekend\n","2025-04-12 17:00:46.120040\n"]}],"source":["# Use this cell to decode the images\n","\n","# The file to read\n","filePath = \"./Generated/image_0.png\"\n","\n","# The word string\n","wordString = \"\"\n","\n","\n","#############################################################################################\n","# Open the image and read the sentence\n","#############################################################################################\n","image = Image.open(filePath)\n","\n","byte_io = io.BytesIO()\n","\n","image.save(byte_io, format='PNG')\n","\n","testBytes = image.tobytes()\n","# print(testBytes)\n","\n","\n","count = 0\n","firstByte = \"\"\n","secondByte = \"\"\n","completeByte = \"\"\n","foundFirstWord = False\n","foundFirstZero = False\n","\n","for byte in testBytes:\n","\n","    if firstByte == \"\":\n","        firstByte = byte << 8\n","        continue\n","\n","    else:\n","        secondByte = byte\n","        completeByte = firstByte + secondByte\n","\n","        #print(int(completeByte))\n","\n","        #secondByte = b''\n","\n","    # The exit condition is if we find two zeros in a row\n","\n","    if (foundFirstWord == True) and (int(completeByte) == 0):\n","        if foundFirstZero == False:\n","           foundFirstZero = True\n","           firstByte = \"\"\n","           continue\n","        else:\n","            break\n","\n","    # print(f\"First byte: {firstByte}; Second Byte {secondByte}; Complete Byte {completeByte}\")\n","\n","    if completeByte >= len(wordBank):\n","      wordString = wordString + \" \" + \"JUNK_WORD_OUT_OF_CONTEXT\"\n","    else:\n","      wordString = wordString + \" \" + reverseIndex[completeByte]\n","\n","    foundFirstWord = True\n","    count += 1\n","    firstByte = \"\"\n","    #print(count)\n","\n","\n","print(wordString)\n","\n","\n","print(datetime.now())\n",""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ddLkJ_yCdD1W"},"outputs":[],"source":[]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.9"},"widgets":{"application/vnd.jupyter.widget-state+json":{"ccd65b514ca04c3aa176bb51bd1be7b7":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c737dc8bd3294d87a8ff5a4030ab91f4","IPY_MODEL_e666312564394e11aab3b63fef319ddf","IPY_MODEL_07737e04105b4636a3854763d9635569"],"layout":"IPY_MODEL_0f558675fdeb4b8bb423728c76906b22"}},"c737dc8bd3294d87a8ff5a4030ab91f4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ca2735f1adee43cca5f492a4db4475ef","placeholder":"​","style":"IPY_MODEL_74e63f3a36bf481fa6b96e5fa0f75bb4","value":"loss: 0.0158: 100%"}},"e666312564394e11aab3b63fef319ddf":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f8a1101aff1e49498157a799275aa6fc","max":100,"min":0,"orientation":"horizontal","style":"IPY_MODEL_70be187c50114b4284b3be858740bd38","value":100}},"07737e04105b4636a3854763d9635569":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3ac3d064483f49bfb291934fa0dacc04","placeholder":"​","style":"IPY_MODEL_3d65d56ae8f04d159a2bba6a20cd055b","value":" 100/100 [04:57&lt;00:00,  2.84s/it]"}},"0f558675fdeb4b8bb423728c76906b22":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca2735f1adee43cca5f492a4db4475ef":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"74e63f3a36bf481fa6b96e5fa0f75bb4":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f8a1101aff1e49498157a799275aa6fc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"70be187c50114b4284b3be858740bd38":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3ac3d064483f49bfb291934fa0dacc04":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3d65d56ae8f04d159a2bba6a20cd055b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"a3994852cc2544e9a3a1e11faa5d7c60":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_441f991917fb4a41a76351b596f75630","IPY_MODEL_db8774b4b2744bd2ac76efba28ab4730","IPY_MODEL_c809daf92bb14a198419e3aa277a17ef"],"layout":"IPY_MODEL_f7f47958e3c74fb38bf239c55b28e97c"}},"441f991917fb4a41a76351b596f75630":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_912e03ee280a462399c3b0f5be8a824c","placeholder":"​","style":"IPY_MODEL_5b35b84d81d840ffa8f5afb7be582767","value":"sampling loop time step: 100%"}},"db8774b4b2744bd2ac76efba28ab4730":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_050171d68a2c4b4387cfeaa704a4be76","max":5,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a6ee20d6db844610b943b3e6c1216640","value":5}},"c809daf92bb14a198419e3aa277a17ef":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3a6b05aabdb64fcfaec520fa8c936332","placeholder":"​","style":"IPY_MODEL_61dc1a14d4a948d1acfa0c2b1f26747c","value":" 5/5 [00:00&lt;00:00, 21.04it/s]"}},"f7f47958e3c74fb38bf239c55b28e97c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"912e03ee280a462399c3b0f5be8a824c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5b35b84d81d840ffa8f5afb7be582767":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"050171d68a2c4b4387cfeaa704a4be76":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a6ee20d6db844610b943b3e6c1216640":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3a6b05aabdb64fcfaec520fa8c936332":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"61dc1a14d4a948d1acfa0c2b1f26747c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}